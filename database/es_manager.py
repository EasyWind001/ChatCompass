"""
Elasticsearchæ•°æ®åº“ç®¡ç†å™¨

å®ç°åŸºäºElasticsearchçš„æ•°æ®å­˜å‚¨å’Œæœç´¢åŠŸèƒ½ã€‚
æ”¯æŒä¸­æ–‡åˆ†è¯ã€å…¨æ–‡æœç´¢ã€å‘é‡æœç´¢ç­‰é«˜çº§åŠŸèƒ½ã€‚

ä½œè€…: ChatCompass Team
ç‰ˆæœ¬: v1.2.2
"""

from typing import List, Dict, Optional, Any, Tuple
from datetime import datetime
from elasticsearch import Elasticsearch, helpers
from elasticsearch.exceptions import NotFoundError, ConnectionError as ESConnectionError
import logging
import os
from .base_storage import BaseStorage

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ElasticsearchManager(BaseStorage):
    """Elasticsearchå­˜å‚¨å®ç°"""

    def __init__(self, host: str = "localhost", port: int = 9200,
                 index_prefix: str = "chatcompass",
                 username: Optional[str] = None,
                 password: Optional[str] = None):
        """
        åˆå§‹åŒ–Elasticsearchè¿æ¥
        
        Args:
            host: ESä¸»æœºåœ°å€
            port: ESç«¯å£
            index_prefix: ç´¢å¼•åç§°å‰ç¼€
            username: ç”¨æˆ·åï¼ˆå¯é€‰ï¼‰
            password: å¯†ç ï¼ˆå¯é€‰ï¼‰
        """
        # æ„å»ºè¿æ¥é…ç½®
        es_config = {
            'hosts': [f'{host}:{port}'],
            'retry_on_timeout': True,
            'max_retries': 3,
            'timeout': 30
        }
        
        # æ·»åŠ è®¤è¯ä¿¡æ¯
        if username and password:
            es_config['http_auth'] = (username, password)
        
        try:
            self.es = Elasticsearch(**es_config)
            
            # æ£€æŸ¥è¿æ¥
            if not self.es.ping():
                raise ESConnectionError("æ— æ³•è¿æ¥åˆ°Elasticsearch")
            
            logger.info(f"âœ… æˆåŠŸè¿æ¥åˆ°Elasticsearch {host}:{port}")
            
        except Exception as e:
            logger.error(f"âŒ Elasticsearchè¿æ¥å¤±è´¥: {e}")
            raise
        
        self.index_prefix = index_prefix
        self.conversation_index = f"{index_prefix}_conversations"
        self.message_index = f"{index_prefix}_messages"
        self.tag_index = f"{index_prefix}_tags"
        
        # åˆå§‹åŒ–ç´¢å¼•
        self._create_indices()
    
    def _create_indices(self):
        """åˆ›å»ºElasticsearchç´¢å¼•å’Œæ˜ å°„"""
        
        # Conversationsç´¢å¼•æ˜ å°„ï¼ˆä½¿ç”¨æ ‡å‡†åˆ†æå™¨ï¼Œä¸ä¾èµ–IKæ’ä»¶ï¼‰
        conversation_mapping = {
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 0,
                "analysis": {
                    "analyzer": {
                        "default": {
                            "type": "standard"
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    "conversation_id": {"type": "keyword"},
                    "source_url": {"type": "keyword"},  # æ·»åŠ source_urlå­—æ®µ
                    "title": {
                        "type": "text",
                        "analyzer": "standard",
                        "fields": {
                            "keyword": {"type": "keyword"}
                        }
                    },
                    "platform": {"type": "keyword"},
                    "create_time": {"type": "date"},
                    "update_time": {"type": "date"},
                    "message_count": {"type": "integer"},
                    "total_tokens": {"type": "integer"},
                    "model": {"type": "keyword"},
                    "tags": {"type": "keyword"},
                    "summary": {
                        "type": "text",
                        "analyzer": "standard"
                    },
                    "category": {"type": "keyword"},
                    "raw_content": {
                        "type": "text",
                        "index": False  # ä¸ç´¢å¼•ï¼Œåªå­˜å‚¨åŸå§‹å†…å®¹
                    }
                }
            }
        }
        
        # Messagesç´¢å¼•æ˜ å°„ï¼ˆä½¿ç”¨æ ‡å‡†åˆ†æå™¨ï¼‰
        message_mapping = {
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 0,
                "analysis": {
                    "analyzer": {
                        "default": {
                            "type": "standard"
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    "message_id": {"type": "keyword"},
                    "conversation_id": {"type": "keyword"},
                    "role": {"type": "keyword"},
                    "content": {
                        "type": "text",
                        "analyzer": "standard"
                    },
                    "create_time": {"type": "date"},
                    "order_index": {"type": "integer"},
                    "parent_message_id": {"type": "keyword"},
                    "tokens": {"type": "integer"}
                }
            }
        }
        
        # Tagsç´¢å¼•æ˜ å°„
        tag_mapping = {
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 0
            },
            "mappings": {
                "properties": {
                    "tag_id": {"type": "keyword"},
                    "name": {
                        "type": "text",
                        "fields": {
                            "keyword": {"type": "keyword"}
                        }
                    },
                    "color": {"type": "keyword"},
                    "description": {"type": "text"},
                    "create_time": {"type": "date"}
                }
            }
        }
        
        # åˆ›å»ºç´¢å¼•
        for index_name, mapping in [
            (self.conversation_index, conversation_mapping),
            (self.message_index, message_mapping),
            (self.tag_index, tag_mapping)
        ]:
            try:
                if not self.es.indices.exists(index=index_name):
                    self.es.indices.create(index=index_name, body=mapping)
                    logger.info(f"âœ… åˆ›å»ºç´¢å¼•: {index_name}")
                else:
                    logger.info(f"ğŸ“‹ ç´¢å¼•å·²å­˜åœ¨: {index_name}")
            except Exception as e:
                logger.error(f"âŒ åˆ›å»ºç´¢å¼•å¤±è´¥ {index_name}: {e}")
                raise
    
    # ==================== å¯¹è¯ç®¡ç† ====================
    
    def save_conversation(self, conversation_id: str, title: str, 
                         platform: str = "chatgpt",
                         source_url: Optional[str] = None,
                         raw_content: Optional[str] = None,
                         create_time: Optional[str] = None,
                         **kwargs) -> bool:
        """ä¿å­˜å¯¹è¯"""
        try:
            doc = {
                "conversation_id": conversation_id,
                "source_url": source_url or "",  # æ·»åŠ source_url
                "raw_content": raw_content or "",  # æ·»åŠ raw_content
                "title": title,
                "platform": platform,
                "create_time": create_time or datetime.now().isoformat(),
                "update_time": datetime.now().isoformat(),
                "message_count": kwargs.get("message_count", 0),
                "total_tokens": kwargs.get("total_tokens", 0),
                "model": kwargs.get("model", ""),
                "tags": kwargs.get("tags", []),
                "summary": kwargs.get("summary", ""),
                "category": kwargs.get("category", "")
            }
            
            self.es.index(
                index=self.conversation_index,
                id=conversation_id,
                body=doc,
                refresh=True
            )
            
            logger.info(f"âœ… ä¿å­˜å¯¹è¯: {conversation_id}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜å¯¹è¯å¤±è´¥: {e}")
            return False
    
    def get_conversation(self, conversation_id: str) -> Optional[Dict]:
        """è·å–å¯¹è¯è¯¦æƒ…"""
        try:
            result = self.es.get(index=self.conversation_index, id=conversation_id)
            conversation = result['_source']
            conversation['id'] = result['_id']  # æ·»åŠ IDå­—æ®µ
            
            # ç»Ÿä¸€å­—æ®µå
            if 'create_time' in conversation and 'created_at' not in conversation:
                conversation['created_at'] = conversation['create_time']
            if 'update_time' in conversation and 'updated_at' not in conversation:
                conversation['updated_at'] = conversation['update_time']
            
            return conversation
        except NotFoundError:
            return None
        except Exception as e:
            logger.error(f"âŒ è·å–å¯¹è¯å¤±è´¥: {e}")
            return None
    
    def list_conversations(self, platform: Optional[str] = None,
                          tags: Optional[List[str]] = None,
                          limit: int = 50,
                          offset: int = 0,
                          sort_by: str = "update_time",
                          order: str = "desc") -> List[Dict]:
        """åˆ—å‡ºå¯¹è¯"""
        try:
            query = {"bool": {"must": []}}
            
            if platform:
                query["bool"]["must"].append({"term": {"platform": platform}})
            
            if tags:
                query["bool"]["must"].append({"terms": {"tags": tags}})
            
            # å¦‚æœæ²¡æœ‰ä»»ä½•æ¡ä»¶ï¼Œä½¿ç”¨match_all
            if not query["bool"]["must"]:
                query = {"match_all": {}}
            
            result = self.es.search(
                index=self.conversation_index,
                body={
                    "query": query,
                    "sort": [{sort_by: {"order": order}}],
                    "from": offset,
                    "size": limit
                }
            )
            
            # è¿”å›æ—¶åŒ…å«æ–‡æ¡£IDï¼Œå¹¶ç»Ÿä¸€å­—æ®µå
            conversations = []
            for hit in result['hits']['hits']:
                conversation = hit['_source']
                conversation['id'] = hit['_id']  # æ·»åŠ IDå­—æ®µ
                
                # ç»Ÿä¸€å­—æ®µåï¼šElasticsearchä½¿ç”¨create_timeï¼Œä½†ä¸»ç¨‹åºæœŸæœ›created_at
                if 'create_time' in conversation and 'created_at' not in conversation:
                    conversation['created_at'] = conversation['create_time']
                if 'update_time' in conversation and 'updated_at' not in conversation:
                    conversation['updated_at'] = conversation['update_time']
                
                conversations.append(conversation)
            
            return conversations
            
        except Exception as e:
            logger.error(f"âŒ åˆ—å‡ºå¯¹è¯å¤±è´¥: {e}")
            return []
    
    def delete_conversation(self, conversation_id: str) -> bool:
        """åˆ é™¤å¯¹è¯"""
        try:
            # åˆ é™¤å¯¹è¯
            self.es.delete(index=self.conversation_index, id=conversation_id, refresh=True)
            
            # åˆ é™¤ç›¸å…³æ¶ˆæ¯
            self.es.delete_by_query(
                index=self.message_index,
                body={"query": {"term": {"conversation_id": conversation_id}}},
                refresh=True
            )
            
            logger.info(f"âœ… åˆ é™¤å¯¹è¯: {conversation_id}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤å¯¹è¯å¤±è´¥: {e}")
            return False
    
    def update_conversation(self, conversation_id: str, **kwargs) -> bool:
        """æ›´æ–°å¯¹è¯ä¿¡æ¯"""
        try:
            update_doc = {key: value for key, value in kwargs.items() if value is not None}
            update_doc["update_time"] = datetime.now().isoformat()
            
            self.es.update(
                index=self.conversation_index,
                id=conversation_id,
                body={"doc": update_doc},
                refresh=True
            )
            
            logger.info(f"âœ… æ›´æ–°å¯¹è¯: {conversation_id}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°å¯¹è¯å¤±è´¥: {e}")
            return False
    
    # ==================== æ¶ˆæ¯ç®¡ç† ====================
    
    def save_message(self, message_id: str, conversation_id: str,
                    role: str, content: str,
                    create_time: Optional[str] = None,
                    **kwargs) -> bool:
        """ä¿å­˜æ¶ˆæ¯"""
        try:
            doc = {
                "message_id": message_id,
                "conversation_id": conversation_id,
                "role": role,
                "content": content,
                "create_time": create_time or datetime.now().isoformat(),
                "order_index": kwargs.get("order_index", 0),
                "parent_message_id": kwargs.get("parent_message_id", ""),
                "tokens": kwargs.get("tokens", 0)
            }
            
            self.es.index(
                index=self.message_index,
                id=message_id,
                body=doc,
                refresh=True
            )
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ¶ˆæ¯å¤±è´¥: {e}")
            return False
    
    def get_messages(self, conversation_id: str,
                    limit: Optional[int] = None) -> List[Dict]:
        """è·å–å¯¹è¯çš„æ‰€æœ‰æ¶ˆæ¯"""
        try:
            query_body = {
                "query": {"term": {"conversation_id": conversation_id}},
                "sort": [{"order_index": {"order": "asc"}}],
                "size": limit or 10000
            }
            
            result = self.es.search(index=self.message_index, body=query_body)
            return [hit['_source'] for hit in result['hits']['hits']]
            
        except Exception as e:
            logger.error(f"âŒ è·å–æ¶ˆæ¯å¤±è´¥: {e}")
            return []
    
    # ==================== æœç´¢åŠŸèƒ½ ====================
    
    def search(self, query: str,
              search_type: str = "full",
              platform: Optional[str] = None,
              tags: Optional[List[str]] = None,
              limit: int = 20,
              offset: int = 0) -> List[Dict]:
        """
        å…¨æ–‡æœç´¢
        
        Args:
            query: æœç´¢å…³é”®è¯
            search_type: æœç´¢ç±»å‹ (full/title/content)
            platform: å¹³å°ç­›é€‰
            tags: æ ‡ç­¾ç­›é€‰
            limit: è¿”å›æ•°é‡
            offset: åç§»é‡
        
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨ï¼ŒåŒ…å«åŒ¹é…çš„å¯¹è¯å’Œæ¶ˆæ¯
        """
        try:
            results = []
            
            # æœç´¢å¯¹è¯æ ‡é¢˜å’Œæ‘˜è¦
            if search_type in ["full", "title"]:
                conv_results = self._search_conversations(
                    query, platform, tags, limit, offset
                )
                results.extend(conv_results)
            
            # æœç´¢æ¶ˆæ¯å†…å®¹
            if search_type in ["full", "content"]:
                msg_results = self._search_messages(
                    query, platform, tags, limit, offset
                )
                results.extend(msg_results)
            
            # æŒ‰è¯„åˆ†æ’åº
            results.sort(key=lambda x: x.get('score', 0), reverse=True)
            
            return results[:limit]
            
        except Exception as e:
            logger.error(f"âŒ æœç´¢å¤±è´¥: {e}")
            return []
    
    def _search_conversations(self, query: str, platform: Optional[str],
                             tags: Optional[List[str]],
                             limit: int, offset: int) -> List[Dict]:
        """æœç´¢å¯¹è¯"""
        try:
            must_clauses = [
                {
                    "multi_match": {
                        "query": query,
                        "fields": ["title^3", "summary^2", "tags"],
                        "type": "best_fields",
                        "operator": "or"
                    }
                }
            ]
            
            if platform:
                must_clauses.append({"term": {"platform": platform}})
            
            if tags:
                must_clauses.append({"terms": {"tags": tags}})
            
            search_body = {
                "query": {"bool": {"must": must_clauses}},
                "highlight": {
                    "fields": {
                        "title": {},
                        "summary": {}
                    },
                    "pre_tags": ["<mark>"],
                    "post_tags": ["</mark>"]
                },
                "from": offset,
                "size": limit
            }
            
            result = self.es.search(index=self.conversation_index, body=search_body)
            
            conversations = []
            for hit in result['hits']['hits']:
                conv = hit['_source'].copy()
                conv['id'] = hit['_id']  # æ·»åŠ IDå­—æ®µ
                conv['score'] = hit['_score']
                conv['search_type'] = 'conversation'
                conv['highlights'] = hit.get('highlight', {})
                
                # ç»Ÿä¸€å­—æ®µå
                if 'create_time' in conv and 'created_at' not in conv:
                    conv['created_at'] = conv['create_time']
                if 'update_time' in conv and 'updated_at' not in conv:
                    conv['updated_at'] = conv['update_time']
                
                conversations.append(conv)
            
            return conversations
            
        except Exception as e:
            logger.error(f"âŒ æœç´¢å¯¹è¯å¤±è´¥: {e}")
            return []
    
    def _search_messages(self, query: str, platform: Optional[str],
                        tags: Optional[List[str]],
                        limit: int, offset: int) -> List[Dict]:
        """æœç´¢æ¶ˆæ¯å†…å®¹"""
        try:
            # å…ˆæœç´¢æ¶ˆæ¯
            search_body = {
                "query": {
                    "match": {
                        "content": {
                            "query": query,
                            "operator": "or"
                        }
                    }
                },
                "highlight": {
                    "fields": {
                        "content": {
                            "fragment_size": 150,
                            "number_of_fragments": 3
                        }
                    },
                    "pre_tags": ["<mark>"],
                    "post_tags": ["</mark>"]
                },
                "from": offset,
                "size": limit
            }
            
            result = self.es.search(index=self.message_index, body=search_body)
            
            messages = []
            for hit in result['hits']['hits']:
                msg = hit['_source'].copy()
                msg['score'] = hit['_score']
                msg['search_type'] = 'message'
                msg['highlights'] = hit.get('highlight', {})
                
                # è·å–æ‰€å±å¯¹è¯ä¿¡æ¯
                conv = self.get_conversation(msg['conversation_id'])
                if conv:
                    # åº”ç”¨å¹³å°å’Œæ ‡ç­¾ç­›é€‰
                    if platform and conv.get('platform') != platform:
                        continue
                    if tags and not any(tag in conv.get('tags', []) for tag in tags):
                        continue
                    
                    msg['conversation_title'] = conv['title']
                    msg['platform'] = conv['platform']
                    messages.append(msg)
            
            return messages
            
        except Exception as e:
            logger.error(f"âŒ æœç´¢æ¶ˆæ¯å¤±è´¥: {e}")
            return []
    
    # ==================== æ ‡ç­¾ç®¡ç† ====================
    
    def save_tag(self, tag_id: str, name: str, color: str = "#3b82f6",
                description: str = "") -> bool:
        """ä¿å­˜æ ‡ç­¾"""
        try:
            doc = {
                "tag_id": tag_id,
                "name": name,
                "color": color,
                "description": description,
                "create_time": datetime.now().isoformat()
            }
            
            self.es.index(
                index=self.tag_index,
                id=tag_id,
                body=doc,
                refresh=True
            )
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ ‡ç­¾å¤±è´¥: {e}")
            return False
    
    def get_all_tags(self) -> List[Dict]:
        """è·å–æ‰€æœ‰æ ‡ç­¾"""
        try:
            result = self.es.search(
                index=self.tag_index,
                body={"query": {"match_all": {}}, "size": 1000}
            )
            
            return [hit['_source'] for hit in result['hits']['hits']]
            
        except Exception as e:
            logger.error(f"âŒ è·å–æ ‡ç­¾å¤±è´¥: {e}")
            return []
    
    def delete_tag(self, tag_id: str) -> bool:
        """åˆ é™¤æ ‡ç­¾"""
        try:
            self.es.delete(index=self.tag_index, id=tag_id, refresh=True)
            return True
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤æ ‡ç­¾å¤±è´¥: {e}")
            return False
    
    # ==================== ç»Ÿè®¡åˆ†æ ====================
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = {}
            
            # å¯¹è¯ç»Ÿè®¡
            conv_count = self.es.count(index=self.conversation_index)
            stats['total_conversations'] = conv_count['count']
            
            # æ¶ˆæ¯ç»Ÿè®¡
            msg_count = self.es.count(index=self.message_index)
            stats['total_messages'] = msg_count['count']
            
            # æ ‡ç­¾ç»Ÿè®¡
            tag_count = self.es.count(index=self.tag_index)
            stats['total_tags'] = tag_count['count']
            
            # å¹³å°ç»Ÿè®¡
            platform_agg = self.es.search(
                index=self.conversation_index,
                body={
                    "size": 0,
                    "aggs": {
                        "platforms": {
                            "terms": {"field": "platform"}
                        }
                    }
                }
            )
            
            stats['by_platform'] = {
                bucket['key']: bucket['doc_count']
                for bucket in platform_agg['aggregations']['platforms']['buckets']
            }
            
            return stats
            
        except Exception as e:
            logger.error(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    # ==================== æ‰¹é‡æ“ä½œ ====================
    
    def bulk_save_messages(self, messages: List[Dict]) -> int:
        """æ‰¹é‡ä¿å­˜æ¶ˆæ¯"""
        try:
            actions = []
            for msg in messages:
                action = {
                    "_index": self.message_index,
                    "_id": msg['message_id'],
                    "_source": msg
                }
                actions.append(action)
            
            success, failed = helpers.bulk(self.es, actions, refresh=True)
            logger.info(f"âœ… æ‰¹é‡ä¿å­˜æ¶ˆæ¯: æˆåŠŸ {success}, å¤±è´¥ {len(failed)}")
            return success
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡ä¿å­˜æ¶ˆæ¯å¤±è´¥: {e}")
            return 0
    
    # ==================== æ•°æ®è¿ç§» ====================
    
    def migrate_from_sqlite(self, sqlite_db_path: str) -> Tuple[int, int]:
        """
        ä»SQLiteè¿ç§»æ•°æ®åˆ°Elasticsearch
        
        Returns:
            (æˆåŠŸå¯¹è¯æ•°, æˆåŠŸæ¶ˆæ¯æ•°)
        """
        import sqlite3
        
        try:
            conn = sqlite3.connect(sqlite_db_path)
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            
            # è¿ç§»å¯¹è¯
            cursor.execute("SELECT * FROM conversations")
            conversations = cursor.fetchall()
            
            conv_count = 0
            for conv in conversations:
                conv_dict = dict(conv)
                if self.save_conversation(**conv_dict):
                    conv_count += 1
            
            # è¿ç§»æ¶ˆæ¯
            cursor.execute("SELECT * FROM messages")
            messages = cursor.fetchall()
            
            msg_list = [dict(msg) for msg in messages]
            msg_count = self.bulk_save_messages(msg_list)
            
            # è¿ç§»æ ‡ç­¾
            cursor.execute("SELECT * FROM tags")
            tags = cursor.fetchall()
            
            for tag in tags:
                tag_dict = dict(tag)
                self.save_tag(**tag_dict)
            
            conn.close()
            
            logger.info(f"âœ… æ•°æ®è¿ç§»å®Œæˆ: {conv_count}ä¸ªå¯¹è¯, {msg_count}æ¡æ¶ˆæ¯")
            return conv_count, msg_count
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®è¿ç§»å¤±è´¥: {e}")
            return 0, 0
    
    # ==================== å¥åº·æ£€æŸ¥ ====================
    
    def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        try:
            cluster_health = self.es.cluster.health()
            
            return {
                "status": cluster_health['status'],
                "cluster_name": cluster_health['cluster_name'],
                "number_of_nodes": cluster_health['number_of_nodes'],
                "active_shards": cluster_health['active_shards'],
                "indices": {
                    "conversations": self.es.count(index=self.conversation_index)['count'],
                    "messages": self.es.count(index=self.message_index)['count'],
                    "tags": self.es.count(index=self.tag_index)['count']
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return {"status": "error", "error": str(e)}
    
    def close(self):
        """å…³é—­è¿æ¥"""
        try:
            self.es.close()
            logger.info("âœ… Elasticsearchè¿æ¥å·²å…³é—­")
        except Exception as e:
            logger.error(f"âŒ å…³é—­è¿æ¥å¤±è´¥: {e}")
    
    # ==================== BaseStorageæŠ½è±¡æ–¹æ³•å®ç° ====================
    
    def connect(self) -> None:
        """å»ºç«‹è¿æ¥ï¼ˆå·²åœ¨__init__ä¸­å®ç°ï¼‰"""
        pass
    
    def is_connected(self) -> bool:
        """æ£€æŸ¥è¿æ¥çŠ¶æ€"""
        try:
            return self.es.ping()
        except Exception:
            return False
    
    def add_conversation(self,
                        platform: str,
                        source_url: str,
                        title: str,
                        summary: str,
                        raw_content: str,
                        category: Optional[str] = None,
                        tags: Optional[List[str]] = None) -> int:
        """æ·»åŠ å¯¹è¯ï¼ˆå…¼å®¹BaseStorageæ¥å£ï¼‰"""
        import hashlib
        import json
        
        # ç”Ÿæˆconversation_id
        conversation_id = hashlib.md5(source_url.encode()).hexdigest()
        
        # è§£æraw_contentè·å–æ¶ˆæ¯
        try:
            content_data = json.loads(raw_content)
            message_count = len(content_data.get('messages', []))
        except:
            message_count = 0
        
        # ä¿å­˜å¯¹è¯
        self.save_conversation(
            conversation_id=conversation_id,
            title=title,
            platform=platform,
            source_url=source_url,  # ä¼ é€’source_url
            raw_content=raw_content,  # ä¼ é€’raw_content
            summary=summary,
            category=category or "",
            tags=tags or [],
            message_count=message_count
        )
        
        return int(conversation_id[:8], 16)  # è¿”å›æ•´æ•°ID
    
    def get_conversation_by_url(self, url: str) -> Optional[Dict[str, Any]]:
        """é€šè¿‡URLè·å–å¯¹è¯"""
        import hashlib
        conversation_id = hashlib.md5(url.encode()).hexdigest()
        return self.get_conversation(conversation_id)
    
    def add_tags(self, conversation_id: int, tags: List[str]) -> None:
        """æ·»åŠ æ ‡ç­¾"""
        conv_id = format(conversation_id, 'x').zfill(8)
        conv = self.get_conversation(conv_id)
        if conv:
            existing_tags = set(conv.get('tags', []))
            existing_tags.update(tags)
            self.update_conversation(conv_id, tags=list(existing_tags))
    
    def remove_tags(self, conversation_id: int, tags: List[str]) -> None:
        """ç§»é™¤æ ‡ç­¾"""
        conv_id = format(conversation_id, 'x').zfill(8)
        conv = self.get_conversation(conv_id)
        if conv:
            existing_tags = set(conv.get('tags', []))
            existing_tags.difference_update(tags)
            self.update_conversation(conv_id, tags=list(existing_tags))
    
    def get_conversation_tags(self, conversation_id: int) -> List[str]:
        """è·å–å¯¹è¯æ ‡ç­¾"""
        conv_id = format(conversation_id, 'x').zfill(8)
        conv = self.get_conversation(conv_id)
        return conv.get('tags', []) if conv else []
    
    def search_conversations(self,
                            keyword: str,
                            limit: int = 50,
                            context_size: int = 100) -> List[Dict[str, Any]]:
        """å…¨æ–‡æœç´¢å¯¹è¯ï¼ˆå…¼å®¹BaseStorageæ¥å£ï¼‰"""
        return self.search(query=keyword, search_type="full", limit=limit)
    
    def advanced_search(self,
                       keyword: Optional[str] = None,
                       platform: Optional[str] = None,
                       category: Optional[str] = None,
                       tags: Optional[List[str]] = None,
                       date_from: Optional[datetime] = None,
                       date_to: Optional[datetime] = None,
                       limit: int = 50) -> List[Dict[str, Any]]:
        """é«˜çº§æœç´¢"""
        try:
            must_clauses = []
            
            if keyword:
                must_clauses.append({
                    "multi_match": {
                        "query": keyword,
                        "fields": ["title^3", "summary^2", "category", "tags"]
                    }
                })
            
            if platform:
                must_clauses.append({"term": {"platform": platform}})
            
            if category:
                must_clauses.append({"term": {"category": category}})
            
            if tags:
                must_clauses.append({"terms": {"tags": tags}})
            
            if date_from or date_to:
                range_query = {"range": {"create_time": {}}}
                if date_from:
                    range_query["range"]["create_time"]["gte"] = date_from.isoformat()
                if date_to:
                    range_query["range"]["create_time"]["lte"] = date_to.isoformat()
                must_clauses.append(range_query)
            
            query = {"bool": {"must": must_clauses}} if must_clauses else {"match_all": {}}
            
            result = self.es.search(
                index=self.conversation_index,
                body={
                    "query": query,
                    "size": limit,
                    "sort": [{"create_time": {"order": "desc"}}]
                }
            )
            
            return [hit['_source'] for hit in result['hits']['hits']]
            
        except Exception as e:
            logger.error(f"âŒ é«˜çº§æœç´¢å¤±è´¥: {e}")
            return []
    
    def optimize(self) -> None:
        """ä¼˜åŒ–å­˜å‚¨ï¼ˆå¼ºåˆ¶åˆ·æ–°å’Œåˆå¹¶ï¼‰"""
        try:
            for index in [self.conversation_index, self.message_index, self.tag_index]:
                self.es.indices.refresh(index=index)
                self.es.indices.forcemerge(index=index, max_num_segments=1)
            logger.info("âœ… ç´¢å¼•ä¼˜åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ ç´¢å¼•ä¼˜åŒ–å¤±è´¥: {e}")
    
    def backup(self, backup_path: str) -> bool:
        """å¤‡ä»½æ•°æ®ï¼ˆå¯¼å‡ºä¸ºJSONï¼‰"""
        import json
        try:
            data = {
                'conversations': self.list_conversations(limit=10000),
                'tags': self.get_all_tags()
            }
            
            with open(backup_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… æ•°æ®å·²å¤‡ä»½åˆ°: {backup_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å¤‡ä»½å¤±è´¥: {e}")
            return False
    
    def export_data(self, export_format: str = 'json') -> str:
        """å¯¼å‡ºæ•°æ®"""
        import json
        try:
            data = {
                'conversations': self.list_conversations(limit=10000),
                'tags': self.get_all_tags()
            }
            return json.dumps(data, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
            return "{}"
    
    def import_data(self, data: str, data_format: str = 'json') -> int:
        """å¯¼å…¥æ•°æ®"""
        import json
        try:
            data_dict = json.loads(data)
            count = 0
            
            # å¯¼å…¥å¯¹è¯
            for conv in data_dict.get('conversations', []):
                if self.save_conversation(**conv):
                    count += 1
            
            # å¯¼å…¥æ ‡ç­¾
            for tag in data_dict.get('tags', []):
                self.save_tag(**tag)
            
            logger.info(f"âœ… å¯¼å…¥å®Œæˆ: {count}ä¸ªå¯¹è¯")
            return count
            
        except Exception as e:
            logger.error(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
            return 0
